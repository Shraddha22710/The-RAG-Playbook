# 03 â€” Retrievers

## ğŸ“Œ What is a Retriever?
A **retriever** is the part of a RAG system that **finds the most relevant chunks** of text (or images, tables, etc.) from your knowledge base.  

Think of it as the **librarian**:  
- You ask a question â†’ retriever looks up the best passages â†’ passes them to the LLM.  

---

## ğŸ”„ Retriever Workflow
1. Query embedding â†’ vector representation of the userâ€™s question.  
2. Vector search â†’ compare query embedding with stored document embeddings.  
3. Top-k results â†’ return the most semantically similar chunks.  
4. LLM â†’ uses those chunks to generate an answer.  

---

## ğŸ§© Types of Retrievers

### 1. Vector Retriever (semantic search)
- Uses embeddings + vector database (FAISS, Pinecone, Qdrant).
- Finds chunks with similar meaning, not just matching keywords.
- âœ… Best for natural language queries.

---

### 2. Keyword Retriever (lexical search)
- Simple keyword match (like BM25, ElasticSearch).
- âœ… Good for exact matching of terms, e.g., â€œClause 12.3â€
- âŒ Misses paraphrases (e.g., â€œterminate contractâ€ vs â€œend agreementâ€).

---

### 3. Hybrid Retriever
- Combines vector + keyword search.
- âœ… More robust: semantic + exact match.
- Common in enterprise systems (Azure Cognitive Search, Weaviate).

---

## âš–ï¸ Comparison

| Retriever Type | Pros | Cons | Example Use |
|----------------|------|------|-------------|
| Vector         | Captures meaning, flexible | Needs embeddings, compute cost | â€œWhatâ€™s the KYC rule for high-value?â€ |
| Keyword        | Fast, precise for exact terms | Fails on paraphrasing | â€œFind Clause 12.3â€ |
| Hybrid         | Best of both worlds | More infra complexity | Compliance + legal search |

---

## ğŸ’» Code Example (FAISS Retriever)
```python
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Create embeddings
model = SentenceTransformer("all-MiniLM-L6-v2")
documents = ["Clause 1: The tenant shall pay rent monthly.",
             "Clause 2: Termination requires 30-day notice."]
embeddings = model.encode(documents)

# Normalize for cosine similarity
embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

# Build FAISS index
index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings.astype("float32"))

# Query
query = "How to end the contract?"
q_emb = model.encode([query])
q_emb = q_emb / np.linalg.norm(q_emb, axis=1, keepdims=True)

D, I = index.search(q_emb.astype("float32"), k=1)
print("Best match:", documents[I[0][0]])
```



ğŸ§  FAQ

â€œIs the retriever part of the LLM?â€
No. Itâ€™s separate. The retriever searches your external knowledge base, then passes context to the LLM.

â€œWhat if the retriever brings wrong info?â€
Then the LLM generates poor answers. Garbage in â†’ garbage out.

â€œHow many chunks should I retrieve?â€
Typical: 3â€“5 chunks. Too few â†’ miss info, too many â†’ exceed context window.

ğŸ¯ Key Takeaway

The retriever is the backbone of RAG.
A great LLM with poor retrieval = useless.
A solid retriever with a decent LLM = reliable system.

Next â†’ Vector Databases


